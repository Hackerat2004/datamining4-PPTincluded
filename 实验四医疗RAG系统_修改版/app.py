import streamlit as st
import time
import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
os.environ['HF_HOME'] = './hf_cache' 
# import evaluation
from chroma_utils import get_chroma_client
from config import (
    EMBEDDING_MODEL_NAME, GENERATION_MODEL_NAME, RERANKING_ENABLED, RERANKING_MODEL_NAME,
    EVAL_DATA_PATH, RECALL_TOP_K, EMBEDDING_DIM, MAX_ARTICLES_TO_INDEX, INDEX_METRIC_TYPE, INDEX_TYPE, INDEX_PARAMS,
    SEARCH_PARAMS, TOP_K, id_to_doc_map
)
# Import functions and config from other modules
from config import (
    DATA_FILE, EMBEDDING_MODEL_NAME, GENERATION_MODEL_NAME, TOP_K, GENERATION_TEMPERATURE,
    MAX_ARTICLES_TO_INDEX, CHROMA_PERSIST_DIR, COLLECTION_NAME, RERANKING_ENABLED, RERANKING_MODEL_NAME, RERANKING_NUM, NUM_CANDIDATES,
    id_to_doc_map # Import the global map
)
from data_utils import load_data
from models import load_embedding_model, load_generation_model, load_reranking_model
# Import the new ChromaDB functions
from chroma_utils import get_chroma_client, setup_chroma_collection, index_data_if_needed, search_similar_documents
from rag_core import generate_answer, no_doc_generate_answer



# æ–°å¢ï¼šåˆå§‹åŒ–å¯¹è¯å†å²å­˜å‚¨
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []



# # åœ¨ä¾§è¾¹æ æ·»åŠ è¯„ä¼°é€‰é¡¹
# st.sidebar.header("è¯„ä¼°åŠŸèƒ½")
# # ä¿®æ”¹è¯„ä¼°æŒ‰é’®çš„å›è°ƒé€»è¾‘
# if st.sidebar.button("è¿è¡Œå¬å›è¯„ä¼°"):
#     st.write(f"å½“å‰è¯„ä¼°æ•°æ®è·¯å¾„ï¼š{EVAL_DATA_PATH}")  # æ–°å¢è·¯å¾„æ˜¾ç¤º
#     # ä½¿ç”¨å·²åˆå§‹åŒ–çš„å…¨å±€å®¢æˆ·ç«¯å®ä¾‹
#     if get_milvus_client() and EMBEDDING_MODEL_NAME:  # Changed to use get_milvus_client()
#         with st.spinner("æ­£åœ¨æ‰§è¡Œå¬å›è¯„ä¼°..."):
#             from evaluation import run_evaluation
#             metrics = run_evaluation(get_milvus_client(), load_embedding_model(EMBEDDING_MODEL_NAME))  # Changed here
            
#             st.subheader("è¯„ä¼°ç»“æœ")
#             # åŠ¨æ€ç”ŸæˆæŒ‡æ ‡æ˜¾ç¤º
#             if metrics:
#                 chart_data = {}
#                 for k in RECALL_TOP_K:
#                     key = f"recall@{k}"
#                     if key in metrics:
#                         st.metric(label=key, value=f"{metrics[key]:.2%}")
#                         chart_data[key] = metrics[key]
                
#                 # ä»…åœ¨æœ‰æ•°æ®æ—¶æ˜¾ç¤ºå›¾è¡¨
#                 if chart_data:
#                     st.bar_chart(chart_data)
#                 else:
#                     st.warning("æœªæ‰¾åˆ°æœ‰æ•ˆè¯„ä¼°æŒ‡æ ‡")
#             else:
#                 st.error("è¯„ä¼°å¤±è´¥ï¼Œæœªç”Ÿæˆä»»ä½•æŒ‡æ ‡")
                
#             # åˆ é™¤ä»¥ä¸‹å†—ä½™çš„å¯è§†åŒ–ä»£ç å—
#             # st.bar_chart({
#             #     "Recall@10": metrics["recall@10"],
#             #     "Recall@20": metrics["recall@20"], 
#             #     "Recall@50": metrics["recall@50"]
#             # })
#     else:
#         st.error("æ— æ³•è¿è¡Œè¯„ä¼°ï¼Œè¯·å…ˆåˆå§‹åŒ–ç³»ç»Ÿ")






# --- Streamlit UI è®¾ç½® ---
# st.set_page_config(layout="wide")
st.title("ğŸ“„ åŒ»ç–— RAG ç³»ç»Ÿ (ChromaDB)")
st.markdown(f"ä½¿ç”¨ ChromaDB, `{EMBEDDING_MODEL_NAME}`, å’Œ `{GENERATION_MODEL_NAME}`ã€‚")

# --- åˆå§‹åŒ–ä¸ç¼“å­˜ ---
# è·å– ChromaDB å®¢æˆ·ç«¯ (å¦‚æœæœªç¼“å­˜åˆ™åˆå§‹åŒ–)
chroma_client = get_chroma_client()

if chroma_client:
    # è®¾ç½® collection (å¦‚æœæœªç¼“å­˜åˆ™åˆ›å»º/åŠ è½½ç´¢å¼•)
    collection_is_ready = setup_chroma_collection(chroma_client)

    # åŠ è½½æ¨¡å‹ (ç¼“å­˜)
    embedding_model = load_embedding_model(EMBEDDING_MODEL_NAME)
    generation_model, tokenizer = load_generation_model(GENERATION_MODEL_NAME)
    reranking_model = load_reranking_model(RERANKING_MODEL_NAME) if RERANKING_ENABLED else None  # åŠ è½½é‡æ’åºæ¨¡å‹

    # # --- æ–°å¢ï¼šè®¾å¤‡ç»Ÿä¸€é…ç½® ---
    # import torch
    # device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
    # # ç§»åŠ¨æ¨¡å‹åˆ°ç›®æ ‡è®¾å¤‡
    # embedding_model = embedding_model.to(device)
    # generation_model = generation_model.to(device)
    # if reranking_model:
    #     reranking_model = reranking_model.to(device)

    # æ£€æŸ¥æ‰€æœ‰ç»„ä»¶æ˜¯å¦æˆåŠŸåŠ è½½
    models_loaded = embedding_model and generation_model and tokenizer

    if collection_is_ready and models_loaded:
        # åŠ è½½æ•°æ® (æœªç¼“å­˜)
        pubmed_data = load_data(DATA_FILE)

        # å¦‚æœéœ€è¦åˆ™ç´¢å¼•æ•°æ® (è¿™ä¼šå¡«å…… id_to_doc_map)
        if pubmed_data:
            indexing_successful = index_data_if_needed(chroma_client, pubmed_data, embedding_model)
        else:
            st.warning(f"æ— æ³•ä» {DATA_FILE} åŠ è½½æ•°æ®ã€‚è·³è¿‡ç´¢å¼•ã€‚")
            indexing_successful = False # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œåˆ™è§†ä¸ºä¸æˆåŠŸ

        st.divider()

        # --- RAG äº¤äº’éƒ¨åˆ† ---
        if not indexing_successful and not id_to_doc_map:
             st.error("æ•°æ®ç´¢å¼•å¤±è´¥æˆ–ä¸å®Œæ•´ï¼Œä¸”æ²¡æœ‰æ–‡æ¡£æ˜ å°„ã€‚RAG åŠŸèƒ½å·²ç¦ç”¨ã€‚")
        else:
            # æ–°å¢ï¼šæ˜¾ç¤ºå¯¹è¯å†å²
            for msg in st.session_state.chat_history:
                with st.chat_message(msg["role"]):
                    st.markdown(msg["content"])

            # æ›¿æ¢åŸå•æ¬¡è¾“å…¥ä¸ºèŠå¤©è¾“å…¥æ¡†
            query = st.chat_input("è¯·æå‡ºå…³äºå·²ç´¢å¼•åŒ»ç–—æ–‡ç« çš„é—®é¢˜:")
            
            if query:
                # è®°å½•ç”¨æˆ·å½“å‰æé—®åˆ°å¯¹è¯å†å²
                with st.chat_message("user"):
                    st.markdown(query)
                st.session_state.chat_history.append({"role": "user", "content": query})

                start_time = time.time()

                history_text = " ".join([
                    f"{msg['role']}: {msg['content']}" 
                    for msg in st.session_state.chat_history[:-1]  # æ’é™¤å½“å‰åˆšæ·»åŠ çš„ç”¨æˆ·æé—®ï¼ˆé¿å…é‡å¤ï¼‰
                    ])

                search_query = f"{history_text} ç”¨æˆ·å½“å‰é—®é¢˜: {query}"

                # st.write(id_to_doc_map)
                # 1. æœç´¢ ChromaDB
                with st.spinner("æ­£åœ¨æœç´¢ç›¸å…³æ–‡æ¡£..."):

                    retrieved_ids, distances = search_similar_documents(chroma_client, search_query, embedding_model)
                    # retrieved_ids, distances = search_similar_documents(milvus_client, search_query, reranking_model)

                if not retrieved_ids:
                    st.warning("åœ¨æ•°æ®åº“ä¸­æ‰¾ä¸åˆ°ç›¸å…³æ–‡æ¡£ã€‚")
                else:
                    # 2. ä»æ˜ å°„ä¸­æ£€ç´¢ä¸Šä¸‹æ–‡
                    retrieved_docs = [id_to_doc_map[id] for id in retrieved_ids if id in id_to_doc_map]

                    if not retrieved_docs:
                         st.error("æ£€ç´¢åˆ°çš„ ID æ— æ³•æ˜ å°„åˆ°åŠ è½½çš„æ–‡æ¡£ã€‚è¯·æ£€æŸ¥æ˜ å°„é€»è¾‘ã€‚")
                    else:
                        # 3. æ–°å¢ï¼šé‡æ’åºæ–‡æ¡£ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                        if RERANKING_ENABLED and reranking_model:
                            with st.spinner("æ­£åœ¨é‡æ’åºæ–‡æ¡£..."):
                                # å‡†å¤‡æŸ¥è¯¢-æ–‡æ¡£å¯¹ï¼ˆæ ¼å¼ï¼š[(query, doc_content), ...]ï¼‰
                                sentence_pairs = [(query, doc['content']) for doc in retrieved_docs]
                                # é¢„æµ‹ç›¸å…³æ€§åˆ†æ•°ï¼ˆåˆ†æ•°è¶Šé«˜è¶Šç›¸å…³ï¼‰
                                scores = reranking_model.predict(sentence_pairs)
                                # æŒ‰åˆ†æ•°é™åºæ’åºæ–‡æ¡£
                                reranked = sorted(zip(retrieved_docs, scores, retrieved_ids), key=lambda x: x[1], reverse=True)
                                retrieved_docs = [item[0] for item in reranked]  # é‡æ–°æ’åºåçš„æ–‡æ¡£åˆ—è¡¨
                                retrieved_ids = [item[2] for item in reranked]    # åŒæ­¥æ›´æ–°IDé¡ºåºï¼ˆå¯é€‰ï¼‰
                                distances = [item[1] for item in reranked]       # ç”¨é‡æ’åºåˆ†æ•°æ›¿æ¢åŸè·ç¦»

                        # 4. æ˜¾ç¤ºæ£€ç´¢ç»“æœ
                        st.subheader("æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ–‡æ¡£:")
                        for i, doc in enumerate(retrieved_docs):
                            # æ˜¾ç¤ºé‡æ’åºåçš„åˆ†æ•°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                            if (i >= RERANKING_NUM): break
                            score_str = f", é‡æ’åºåˆ†æ•°: {distances[i]:.4f}" if RERANKING_ENABLED else ""
                            with st.expander(f"æ–‡æ¡£ {i+1} (ID: {retrieved_ids[i]}{score_str}) - {doc['title'][:60]}"):
                                st.write(f"**æ ‡é¢˜:** {doc['title']}")
                                st.write(f"**æ‘˜è¦:** {doc['abstract']}") # å‡è®¾ 'abstract' å­˜å‚¨çš„æ˜¯æ–‡æœ¬å—

                        st.divider()

                        # 3. ç”Ÿæˆç­”æ¡ˆ
                        st.subheader("ç”Ÿæˆçš„ç­”æ¡ˆ:")
                        with st.spinner("æ­£åœ¨æ ¹æ®ä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ..."):
                            # ä¿®æ”¹ï¼šæ–°å¢ chat_history å‚æ•°ä¼ é€’
                            answer = generate_answer(query, retrieved_docs, generation_model, tokenizer, st.session_state.chat_history)
                            response = answer
                        
                        # with st.spinner("æ­£åœ¨æ ¹æ®ä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ..."):
                        #     # ä¼ é€’æ–°å¢çš„å¤šæ ·æ€§æ§åˆ¶å‚æ•°
                        #     answer_candidates = generate_answer(
                        #         query, 
                        #         retrieved_docs, 
                        #         generation_model, 
                        #         tokenizer, 
                        #         st.session_state.chat_history,
                        #         num_candidates=NUM_CANDIDATES,
                        #         temperature=GENERATION_TEMPERATURE
                        #     )
                        #     # é€‰æ‹©æ’åºåçš„ç¬¬ä¸€ä¸ªä½œä¸ºä¸»å›ç­”ï¼ˆä¹Ÿå¯éšæœºé€‰æ‹©ï¼‰
                        #     response = answer_candidates[0]


                        # è®°å½•åŠ©ç†å›ç­”åˆ°å¯¹è¯å†å²å¹¶æ˜¾ç¤º
                        with st.chat_message("assistant"):
                            st.markdown(response)
                        st.session_state.chat_history.append({"role": "assistant", "content": response})
                        
                        with st.spinner("æ­£åœ¨ç”Ÿæˆæ— ä¸Šä¸‹æ–‡å›ç­”..."):
                        # ä¼ å…¥ç©ºæ–‡æ¡£åˆ—è¡¨è¡¨ç¤ºæ— ä¸Šä¸‹æ–‡
                            answer = no_doc_generate_answer(query, generation_model, tokenizer, st.session_state.chat_history)
                        # è®°å½•å¹¶æ˜¾ç¤ºå¯¹ç…§å›ç­”
                        with st.chat_message("assistant"):
                            st.markdown(answer)
                        
                        #  # æ–°å¢ï¼šæ˜¾ç¤ºæ‰€æœ‰å€™é€‰ç­”æ¡ˆï¼ˆå¯é€‰ï¼‰
                        # with st.expander(f"æŸ¥çœ‹å…¨éƒ¨ {NUM_CANDIDATES} ä¸ªå€™é€‰ç­”æ¡ˆ"):
                        #     for i, candidate in enumerate(answer_candidates):
                        #         st.markdown(f"**å€™é€‰ {i+1}:** {candidate}")

                        end_time = time.time()
                        st.info(f"æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’")

    else:
        st.error("åŠ è½½æ¨¡å‹æˆ–è®¾ç½® ChromaDB collection å¤±è´¥ã€‚è¯·æ£€æŸ¥æ—¥å¿—å’Œé…ç½®ã€‚")
else:
    st.error("åˆå§‹åŒ– ChromaDB å®¢æˆ·ç«¯å¤±è´¥ã€‚è¯·æ£€æŸ¥æ—¥å¿—ã€‚")


# --- é¡µè„š/ä¿¡æ¯ä¾§è¾¹æ  ---
st.sidebar.header("ç³»ç»Ÿé…ç½®")
st.sidebar.markdown(f"**å‘é‡å­˜å‚¨:** ChromaDB")
st.sidebar.markdown(f"**æ•°æ®è·¯å¾„:** `{CHROMA_PERSIST_DIR}`")
st.sidebar.markdown(f"**Collection:** `{COLLECTION_NAME}`")
st.sidebar.markdown(f"**æ•°æ®æ–‡ä»¶:** `{DATA_FILE}`")
st.sidebar.markdown(f"**åµŒå…¥æ¨¡å‹:** `{EMBEDDING_MODEL_NAME}`")
st.sidebar.markdown(f"**ç”Ÿæˆæ¨¡å‹:** `{GENERATION_MODEL_NAME}`")
st.sidebar.markdown(f"**æœ€å¤§ç´¢å¼•æ•°:** `{MAX_ARTICLES_TO_INDEX}`")
st.sidebar.markdown(f"**æ£€ç´¢ Top K:** `{TOP_K}`")